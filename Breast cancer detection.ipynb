{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ed2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "INPUT_DATASET = \"datasets/original\"\n",
    "BASE_PATH = \"datasets/idc\"\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c392309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set\n",
      "Building validation set\n",
      "Building testing set\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import random\n",
    "\n",
    "originalPaths = list(paths.list_images(INPUT_DATASET))\n",
    "random.seed(7)\n",
    "random.shuffle(originalPaths)\n",
    "index = int(len(originalPaths) * TRAIN_SPLIT)\n",
    "trainPaths = originalPaths[:index]\n",
    "testPaths = originalPaths[index:]\n",
    "index = int(len(trainPaths) * VAL_SPLIT)\n",
    "valPaths = trainPaths[:index]\n",
    "trainPaths = trainPaths[index:]\n",
    "datasets = [(\"training\", trainPaths, TRAIN_PATH),\n",
    "            (\"validation\", valPaths, VAL_PATH),\n",
    "            (\"testing\", testPaths, TEST_PATH)\n",
    "            ]\n",
    "for (setType, originalPaths, basePath) in datasets:\n",
    "    print(f'Building {setType} set')\n",
    "    if not os.path.exists(basePath):\n",
    "        print(f'Building directory {basePath}')\n",
    "        os.makedirs(basePath)\n",
    "    for path in originalPaths:\n",
    "        file = path.split(os.path.sep)[-1]\n",
    "        label = file[-5:- 4]\n",
    "        labelPath = os.path.sep.join([basePath, label])\n",
    "        if not os.path.exists(labelPath):\n",
    "            print(f'Building directory {labelPath}')\n",
    "            os.makedirs(labelPath)\n",
    "        newPath = os.path.sep.join([labelPath, file])\n",
    "        shutil.copy2(path, newPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5214d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ARPITA N YALIGETI\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "class CancerNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = keras.models.Sequential()\n",
    "        shape = (height, width, depth)\n",
    "        channelDim = -1\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            input_shape = (depth, height, width)\n",
    "            channelDim = 1\n",
    "        model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=shape))\n",
    "        model.add(tf.keras.layers.BatchNormalization(axis=channelDim))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization(axis=channelDim))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization(axis=channelDim))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization(axis=channelDim))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(units=classes, activation='softmax'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687b5887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199818 images belonging to 2 classes.\n",
      "Found 199818 images belonging to 2 classes.\n",
      "Found 22201 images belonging to 2 classes.\n",
      "Found 55505 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "6245/6245 [==============================] - 2560s 409ms/step - loss: 0.3721 - accuracy: 0.8422 - val_loss: 0.4886 - val_accuracy: 0.7519\n",
      "Epoch 2/2\n",
      "6245/6245 [==============================] - 1209s 194ms/step - loss: 0.3353 - accuracy: 0.8562 - val_loss: 0.4938 - val_accuracy: 0.7690\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "training_set = train_datagen.flow_from_directory('datasets/idc/training', target_size=(64, 64), batch_size=32,\n",
    "                                                 class_mode='binary')\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "trainPaths = list(paths.list_images(TRAIN_PATH))\n",
    "lenTrain = len(trainPaths)\n",
    "lenVal = len(list(paths.list_images(VAL_PATH)))\n",
    "lenTest = len(list(paths.list_images(TEST_PATH)))\n",
    "\n",
    "trainLabels = [int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
    "trainLabels = to_categorical(trainLabels)\n",
    "classTotals = trainLabels.sum(axis=0)\n",
    "classWeight = classTotals.max() / classTotals\n",
    "\n",
    "trainAug = ImageDataGenerator(rescale=1 / 255.0, rotation_range=20, zoom_range=0.05, width_shift_range=0.1,\n",
    "height_shift_range=0.1, shear_range=0.05, horizontal_flip=True, vertical_flip=True,\n",
    "                              fill_mode=\"nearest\")\n",
    "\n",
    "valAug = ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "trainGen = trainAug.flow_from_directory(TRAIN_PATH, class_mode=\"categorical\", target_size=(48, 48), color_mode=\"rgb\",\n",
    "                                        shuffle=True, batch_size=BS)\n",
    "valGen = valAug.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)\n",
    "testGen = valAug.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)\n",
    "model = CancerNet.build(width=48, height=48, depth=3, classes=2)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "M = model.fit(x=trainGen, validation_data=valGen, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93e89e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now evaluating the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARPITA N YALIGETI\\AppData\\Local\\Temp\\ipykernel_11552\\1480503738.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  pred_indices = model.predict_generator(testGen, steps=(lenTest // BS) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.98      0.86     39736\n",
      "           1       0.85      0.26      0.40     15769\n",
      "\n",
      "    accuracy                           0.78     55505\n",
      "   macro avg       0.81      0.62      0.63     55505\n",
      "weighted avg       0.79      0.78      0.73     55505\n",
      "\n",
      "[[39022   714]\n",
      " [11704  4065]]\n",
      "Accuracy: 0.776272407891181\n",
      "Specificity: 0.2577842602574672\n",
      "Sensitivity: 0.9820314072881015\n"
     ]
    }
   ],
   "source": [
    "print(\"Now evaluating the model\")\n",
    "testGen.reset()\n",
    "pred_indices = model.predict_generator(testGen, steps=(lenTest // BS) + 1)\n",
    "\n",
    "pred_indices = np.argmax(pred_indices, axis=1)\n",
    "\n",
    "print(classification_report(testGen.classes, pred_indices, target_names=testGen.class_indices.keys()))\n",
    "\n",
    "cm = confusion_matrix(testGen.classes, pred_indices)\n",
    "total = sum(sum(cm))\n",
    "accuracy = (cm[0, 0] + cm[1, 1]) / total\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "print(cm)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print(f'Sensitivity: {sensitivity}')\n",
    "\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), M.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), M.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), M.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), M.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on the IDC Dataset\")\n",
    "plt.xlabel(\"Epoch No.\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a31ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b84bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
